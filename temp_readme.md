# FileMaker Database Transformation Tool

This tool was designed to convert the IPEX FileMaker database into a single CSV file in an appropriate format to be imported into Uncountable.

Running this tool from the CLARND server is incredibly slow, so it is recommended to copy this tool onto your local system and run it from there.

## Table of Contents
2. [Running the Tool](#running-the-tool)
3. [Use](#use)
4. [Functional Modifications](#functional-modifications)
    - [Chronological Processing Stages](#chronological-processing-stages)
    - [Instructions for Anticipated Potential Changes](#instructions-for-anticipated-potential-changes)

## Running the Tool
I've compiled the tool down to a windows executable for ease of use, which can be found in the `bin` folder. Two binaries have been provided, a standalone exe (`FileMaker Data Transformer Standalone.exe`) and a packaged exe (`FileMaker Data Transformer.exe`). The standalone executable takes longer to launch, but the packaged executable needs to stay in the folder it's in (or more specifically, it needs to be in the same folder as the `_internal` folder). I haven't noticed any difference in performance, so either is okay to use.

## Use
The "FileMaker Database Path" entry may take any .CSV or .XLSX file containing the properly formatted FileMaker information. If the XLSX file has multiple worksheets, you will be prompted to enter the name of the sheet you'd like to use. Note that for large databases, XLSX processing takes an appreciable amount of time and using a CSV is recommended. In addition, some unicode characters may cause issues when processing an XLSX which seem to be solved by exporting the file as a CSV.

The input file may have empty or header rows before, during or after the data. Only rows with integer record numbers are processed as data.

The output file is written to the specified output folder, with the name `processed_data.csv`. A log file is also written for troubleshooting. If the status message says the program finished with errors, the record numbers of problematic entries can be found in this log.

## Functional Modifications
Note that throughout this section, unless otherwise specified, "condition" refers to either condition parameter **or** metadata. Process parameters are referred to by that name.
### Chronological Processing Stages
Below is a table listing the major stages that the tool goes through to convert the database to the format that Uncountable needs it in. For each stage, the file(s) and function/class that handle it are listed, along with a general description of how it works.

|      Feature      |      File      |       Function/Class      |             Description             |
|-------------------|----------------|---------------------------|-------------------------------------|
|Beginning and control of transformation process|`gui.py`|`GUIWindow.execute_transformation(self, target_file, out_folder, statii, event)`|All the functionality that does the actual processing and rewriting of the data is in this function. It's not called directly from the button click, but is started in a new thread (`start_execute(...)`)|
|Parsing a row from the FileMaker database|`include/fm_entry.py`|`Entry.parse(self)`|The row containing an entry is split into cells, and all the necessary information is assigned to various properties of the Entry object. Some parsing is done, and specific information can be found in the comments.|
|Re-mapping the FileMaker data to the form it needs to be in for Uncountable|`record.py`|`Record.__init__(self, e: Entry, output_path)`|When a `Record` object is created, the FileMaker data gets remapped as required by uncountable. Outputs are stored in a dictionary (see the next row of the table), while all conditions are stored under a single property (see the second next row of this table). Some other metadata is stored as individual properties.|
|Storing output values in the `Record` object|`output_parameters.py`|`OutputType`|While the dictionary storing each output value is in the function described in the row above, the specific types of outputs are stored here. If outputs need to be added, this is the place to do it. In addition, the string in the output CSV associated with each output is the value of each type defined here.|
|Storing the processing conditions/metadata/etc|`output_parameters.py` / `condition_set.py`|Many classes / `ConditionSet`|Each "condition" is stored as its own class, having a property for its type (Condition, Metadata or Process Parameter) and another for its value. If any processing needs to be done, it is done in one of these classes (see `IzodDimensions` for an example). The condition set is assigned and processed when a `ConditionSet` is created.|
|Writing a `Record` to the CSV file|`record.py`|`Record.write_all_to_one_file`|This function takes a dicationary with all record numbers as keys, and the associated Record objects as values. There are a couple distinct sections. First, it writes the first few header rows (lines 96-129 as I write this). After that, it goes through each record individually, writing the output name, values and the relevant conditions.|

### Instructions for Anticipated Potential Changes
Below is a table with changes that may need to be made to the way the tool processes the data, along with instructions on how to make the desired change.

|Modification|Instructions|
|------------|------------|
|Add new output|To add an output, first add a new member to `OutputType` in `output_parameters.py` whose value is what you want the name of the output in the export file to be. Then, in `Record.__init__` in `record.py`, modify the `self.outputs` dict to add the output to the object (if you're adding a new output, you're likely pulling some new data from the FileMaker database; see the next entry in the table for how to do this). Finally, make sure to add handling for how to write the output to the file in `Record.get_entry_lists`. To see a simple example of how to do this, see the code block following the `# Color L,a,b value` comment. The basic structure is: checking if the output actually has data (`if add_output(...)`) → Getting the default row (`Record.get_std_full_row(...)`) → Populating any conditions along with the output value → appending a copy of the list to the return list (`rows.append(current_row[:])`).|
|Pull new data from the input FileMaker database|To pull in new data from the input CSV/spreadsheet, first add a new member to `EntryColumnIndex` in `fm_entry.py`. The value of this member should be the **index** of the column where the data appears (column number-1). Then, all you need to do is parse and attach the data somewhere in `Entry.parse`. If the value in the cell is exactly the value you need, this probably looks like `self.new_property = self.row[EntryColumnIndex.NewMember]`, but additional processing may also be done. Now, to add this to an output, see the previous table entry. To add this to a condition parameter/metadata, see the next table entry.|
|Add new condition parameters or metadata|All condition/process parameters and metadata are their own class. Start by creating a new one for the desired condition in `output_parameters.py` (`Pendulum` is a good example of a simple condition class). Make sure the `type` property specifies which type out of `ParameterType` the condition is, and that the `value` property specifies its value (unless you're doing some more complicated processing - see the `IzodDimensions` class for an example of this). Next, attach the condition to the `ConditionSet` class in `condition_set.py`. This is pretty straightforward, simply add a new argument in `ConditionSet.__init__`, and a new key-value pair to the `conditions` property. Make sure you also update `Record.condition_set` in `record.py` to actually attach the condition to the record as well (if you're adding a new condition, you're probably pulling in new data from the FileMaker database; see the previous table entry for how to do this). \\ Now that the condition is in the condition set of the record, we need to add a new column in the output CSV to report the condition value. Start by incrementing the `num_of_headers` in `Record.write_all_to_one_file`. The header title is automatically generated using `for e in ConditionSet().conditions`, so you don't need to worry about doing that manually, however, if the condition is more complicated, you may need to make some changes (see how the sample thicknesses were handled beneath that `for` snippet). Finally, in `Record.get_entry_lists`, write the condition in the appropriate column. This probably involves adding a member to `ConditionPosition` whose value is the **index** of the column of the condition in the output file, then adding a line like `current_row[ConditionPosition.NewMember] = record.condition_set.conditions[NewConditionClass]`.|
|Implement process parameters|This program was originally designed to support the use of process parameters, so the code exists, it just needs to be re-implemented. First, in `condition_set.py`, uncomment all the process parameters. The process parameters listed in `ConditionSet.conditions` are already being processed, so they will automatically be attached to that condition set. Now, the process parameter can be accessed through `Record.condition_set.conditions[ProcessParameterClass]`. To either add it as a condition or a new output, see the "Add new condition parameters or metadata" or "Add new output" sections of this table, respectively.|
